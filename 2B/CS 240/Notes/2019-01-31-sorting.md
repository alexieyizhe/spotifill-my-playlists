__CS 240 |__ January 31, 2019

#Lower Bounds of Sorting

There is one more version of `quick-sort` that has a theoretical worst case of $\theta(n \log n)$:

`quick-sort3`: $\Theta(n \log n)$ worst case, not $\Theta(n^2)$
Given an array $A$ of size $n$, choose the median of medians as your pivot.

- Group elements in $A$ into blocks of 5.
- Find median of each block.
- Take median of all the medians as the pivot.

This guarantees the `pivot` will be 'close enough' to the middle that there will be elements $\leq$ `pivot` and elements $\geq$ `pivot`. However, the overhead (number of operations in each iteration) for computing this pivot is far too large to be efficient, and this is _not_ used outside of being theorized.



The lower bounds of **comparison-based sorting** is $\Omega(n \log n)$. However, **non-comparison-based sorting** can achieve $O(n)$ under some _restrictions._



### The Comparison Model

In the comparison model, data can only be accessed in two ways: 

1. Comparing two elements
2. Moving elements around (copying, swapping)

All the sorting algorithms we've seen so far use the comparison model.

When we're using the comparison model, we can think of it like a decision binary tree where each node determines two elements that we want to compare, and its 2 children are the decisions we follow whether the first element is < or > the second. 

![Image result for comparison model decision tree](assets/kQTLG.gif)

In this case, given $n$ elements, we have $n!$ leaves as possible outcomes. Since every internal node has 2 children, we need $n! - 1$ internal nodes. Our height of the tree is then:
$$
\begin{align*}
log(n!) &= \log n + \log(n - 1) + \log(n-2 ) + ...+\log 1\\
&\geq \log n + ... + \log \frac n 2 \\
&\geq \log \frac n 2 + ... + \log \frac n 2 \\
&= \frac n 2 \log \frac n 2 = \frac n 2 \log n - \frac n 2 \in \Omega(n \log n)
\end{align*}
$$
==**Possible exam question**==



##Non-comparison-based Sorting

#### Single-digit Bucket Sort

In an environment, numbers are stored in base $R$ (where $R$ is known as the radix). We can allow each number to have the same number of digits by padding smaller numbers.

We can sort an array of numbers by their last digit. Looking at individual digits from least significant upwards, we do the following:

1. Create buckets where each bucket #$x$ only contains numbers that have the last digit $x$.
2. Pass through the array $A$ and place items in their corresponding bucket.
3. Place the items into the buckets back into the array $A$.

Because this passes over the array just once, this is $O(n)$ efficiency. Note that this does use up more space for the buckets.

![image-20190131092957538](assets/image-20190131092957538.png)

More importantly, this sorting algorithm is **stable.** This means that items that are equivalent according to the sorting method stay in the same order. In terms of bucket sort, this means that elements in the 'same bucket' will maintain their order relative to each other in the original array.



#### Count Sort

Notice that in BucketSort, we didn't need the linkedlists that stored all the elements in each bucket; we just needed the count of the number of elements that belong in each bucket.



#### MSD Radix Sort

This calls CountSort recursively, starting with the Most Significant Digit (on the left) and collecting them into buckets, then recursively calling itself on each individual bucket to sort completely.

This works on elements with variable length, like sorting words in a dictionary. However, the **downside** to this approach is that there are many recursions.



#### LSD Radix Sort

This sorting algorithm is similar to MSD Radix Sort, but starts with the least significant digit. This sorting method is still **stable.**

