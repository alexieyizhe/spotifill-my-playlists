__CS 240 |__ January 29, 2019

# Randomized Algorithms (cont.)

Instead of having the runtime depend on something we can't control (the _input_), we shift the dependency of costs to something we _can_ control: the random numbers generated by us.

However, we need the random numbers to be _based on the input,_ since we don't want two instances of running the algorithm on the _same_ input having _different_ runtimes.

### Expected Running Time of a Randomized Algorithm

Define $T(I, R)​$ to be the running time of the randomized algorithm for an instance $I​$ and the sequence of random numbers $R​$. 

The **expected running time $T^{(exp)}(I)$** for an instance $I$ is the expected value for $T(I, R)$. 

If $I$ is an input $A = [n, 1, 1, ..., 1] $ ($n - 1$ ones and 1 $n$):
$$
\begin{align*}
T^{(exp)}(I) = E[T(I, R)] &= \sum\limits_R T(I, R) \cdot Pr[R] \\
&= \sum\limits_{r = 0, ..., n - 1} T(I, r) \times Pr[k] \\
&= \sum\limits_{r = 0, ..., n - 1} \times \frac{1}{n} \\
&= (n) \frac{1}{n} + 1 \times \frac 1 n + ... + 1 \times \frac 1 n \\
&= 1 \times \frac{n-1} n + n(\frac 1 n)  \\
&= \frac{2n-1} n \lt 2
\end{align*}
$$
The **worst case expected running time** of all inputs of a size $n$ is the _maximum_ of all expected runtimes of each input.

The **average-case expected running time** is $\frac{1}{L} \sum\limits_R T(I, R) \cdot Pr[R] ​$. 

### Randomized QuickSelect

For the QuickSelect algorithm, we now change the pivot selection to be a _random_ index. The probability the random pivot has index $i$ is $\frac 1 n$, so the analysis is the same for the average case. The expected running time is again $\Theta(n)$.

### QuickSort

This is a sorting method based on partitioning.

```pseudocode
A is array of size n

quick-sort1(A): 
	if n <= 1 then return
	p <- choose-pivot1(A)
	i <- partition(A, p)
	quick-sort1(A[0, 1, ... , i - 1])
	quick-sort1(A[i + 1, ... , n - 1])
```

**Worst case:** $T^{(worst)}(n)$ = $T^{(worst)}(n - 1) + \Theta(n)$. Same as **quick-select1** with runtime of $\Theta(n^2)$.

**Best case: $T^{(best)}(n) = T^{(best)}(\lfloor\frac{n-1} 2 \rfloor) + T^{(best)}(\lfloor\frac{n-1} 2 \rfloor) + \Theta(n)$**. Similar to **merge-sort** with runtime of $\Theta(n \log n)$.

#### Analyzing Best Case Runtime: Recursion Tree

![Image result for quicksort recursion tree](assets/158_a.gif)

The above is a recursion tree for **quicksort.** 

We can see that the height of the tree (depth of our recursion) is $\log n$, and since in each level, we're doing $\Theta(n)$ 'amount' of work, our runtime in the _best case_ scenario (where each array is divided evenly into two partitions) is $\Theta(n \log n)$. 

####Analyzing Best Case Runtime: Recurrence Relations

Alternatively, you could find the runtime in the _best case_ scenario from the **recurrence relation**:
$$
\begin{align*}
T(n) &= 2T(\frac n 2) + cn \\
     &= 2[2T(\frac n 4) + c(\frac n 2)] + cn \\
     &= 2[2[2T(\frac n 8) + c(\frac n 4)] + c(\frac n 2)] + cn \\
     &... \\
     &= c\frac n {something} + ... + c\frac n 2 + cn \ \  (\log n \text{ times)} \\
     &\in \Theta(n \log n)
\end{align*}
$$


####Analyzing Average Case Runtime

As before, $\frac 1 n$ of permutations have pivot-index $i$. 

The recursive work is then $T^{(avg)}(i) + T^{(avg)}(n - i - 1)$. So, the average running time is:
$$
T^{(avg)}(n) = cn + \frac 1 n \sum\limits^{n - 1}_{i = 0}[T^{(avg)}(i) + T^{(avg)}(n - i - 1)], \ \ \ \ n \geq 2
$$
If we looked at a recursion tree for this relation, we can see that on each level, we're always doing $c *i + c *(n - i - 1)$ amount of work, which is still $\Theta(n)$ runtime for that single recursive level. 

So, to find the average runtime, we need to multiply it by the average number of recursive calls, which is the average height of the recursion tree, denoted $H(n)$ and given by:
$$
H(n) = 1 + \frac 1 n \sum\limits_{i = 0}^{n - 1} max[H(i), H(n - i - 1)], \ \ \text{if } n \geq 2 \\
H(n) = 0, \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \text{if } n = 1
$$
**Proof (similar to $O$ for QuickSelect)**

[`...`==……...==`...`]  $\frac 1 2$ of the time, $i$ in the wings (denoted with `...`) and the other half, its in the middle (==...==).

​         $\frac 1 4 n$         $\frac 3 4 n$

So,
$$
\begin{align*}
H(n)  &\leq 1 + \frac 1 2 H(\frac 3 4 n) + \frac 1 2 H(n) \\
2H(n) &\leq 2 + H(\frac 3 4 n) + H(n) \\
H(n)  &\leq 2 + H(\frac 3 4 n) \leq 2 + 2 + H(\frac 9 {16} n) \leq \ ... \ \leq 2h \\ &\text{where $h$ is the minimal such that } {\frac 3 4}^h * n \lt 2
\end{align*}
$$
Essentially, $h$ will be the amount of times we can multiply $n$ by $\frac 3 4$ until we hit the base case. If we rearrange and isolate for $h$, we see that $H(n) \in O(\log n)$. 

At each level, we're always doing $\Theta(n)$ amount of work. So, the upper bound of our **average case runtime** is $O(n \log n)$, and our best case runtime is $\Theta(n \log n)$. As a result, our **average case runtime** is also $\Theta(n \log n)$. 

**The proof is the SAME for $T^{(exp)}(n)$.** So, if we randomize the choice of our pivot, our expected runtime is still $\Theta(n \log n)$. 

#### More Notes on QuickSort

- The base case can be increased to stop at an $n \gt 1$, since at that point, we can use insertion sort or the rest of the array. This is because insertion sort, while being $O(n^2)$, is faster up to an array of size $n$, so we use it to speed up our sorting:

  ![Image result for quicksort vs insertion sort](assets/quicksort-vs-insertion-sort.gif)

  